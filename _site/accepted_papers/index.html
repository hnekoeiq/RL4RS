<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Accepted Papers | Workshop on RBFM</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Accepted Papers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models." />
<meta property="og:description" content="Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models." />
<link rel="canonical" href="http://localhost:4000/accepted_papers/" />
<meta property="og:url" content="http://localhost:4000/accepted_papers/" />
<meta property="og:site_name" content="Workshop on RBFM" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Accepted Papers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models.","headline":"Accepted Papers","url":"http://localhost:4000/accepted_papers/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Workshop on RBFM" /></head>
<body><header class="site-header" role="banner">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Workshop on RBFM</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path
                            d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
                    </svg>
                </span>
            </label>

            <div class="trigger"><a class="page-link" href="/accepted_papers/">Accepted Papers</a><a class="page-link" href="/call_for_paper/">Call For Papers</a><a class="page-link" href="/schedule/">Schedule</a><a class="page-link" href="/speakers/">Speakers</a></div>
        </nav></div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Accepted Papers</h1>
  </header>

  <div class="post-content">
    <h2>Long Papers</h2>
<ol>
  <li>
    <p><a href="https://openreview.net/forum?id=jEqdmQYfgg">[Oral]</a> <b>When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?</b><br />
    <!-- Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristobal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, Tony Tong Wang, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez<br/> --></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=Ri2qdOk3Hx">[Oral]</a> <b>LLAVAGUARD: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment</b><br />
    <!-- Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, Patrick Schramowski<br/> --></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=L1vaMp6iBC">[Oral]</a> <b>Multimodal Situational Safety</b><br />
    <!-- Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Anderson Compalas, Xin Eric Wang<br/> --></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=htCZ3fEdV7">[Oral]</a> <b>MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs</b><br />
    <!-- Wenqian Ye, Guangtao Zheng, Yunsheng Ma, Xu Cao, Bolin Lai, James Matthew Rehg, Aidong Zhang<br/> --></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=qkmMvLckB9">[Poster]</a> <b>Skipping Computations in Multimodal LLMs</b><br />
    <!-- Mustafa Shukor, Matthieu Cord<br/> --></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=7DpNGceUUV">[Poster]</a> <b>Coordinated Robustness Evaluation Framework for Vision Language Models</b><br />
    <!-- Ashwin Ramesh Babu, Sajad Mousavi, Desik Rengarajan, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar<br/> --></p>
  </li>

  
  <li>
    <p><a href="https://openreview.net/forum?id=iSL0FHZStr">[Poster]</a> <b>Building and better understanding vision-language models: insights and future directions</b><br /></p>
  </li>
  
  <li>
    <p><a href="https://openreview.net/forum?id=1fpjV6xQ6Q">[Poster]</a> <b>Incorporating Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=2jdohv2bPk">[Poster]</a> <b>CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=Bjm1MzLVRy">[Poster]</a> <b>LEMoN: Label Error Detection using Multimodal Neighbors</b><br /></p>
  </li>
  
  <li>
    <p><a href="https://openreview.net/forum?id=mKFBNMdNj1">[Poster]</a> <b>MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models</b><br /></p>
  </li>
  
  <li>
    <p><a href="https://openreview.net/forum?id=hSsgI1gpZ6">[Poster]</a> <b>Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models</b><br /></p>
  </li>
  
  <li>
    <p><a href="https://openreview.net/forum?id=7IXrDfuJTm">[Poster]</a> <b>Decompose, Recompose, and Conquer: Multi-modal LLMs are Vulnerable to Compositional Adversarial Attacks in Multi-Image Queries</b><br /></p>
  </li>
  
  <li>
    <p><a href="https://openreview.net/forum?id=wZ4yPytmeK">[Poster]</a> <b>Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=UTgNFcpk0j">[Poster]</a> <b>BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=uxUhUiPfQ4">[Poster]</a> <b>MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=VspjUVQsai">[Poster]</a> <b>How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=BSdvfyuLaZ">[Poster]</a> <b>GUIDE: A Responsible Multimodal Approach for Enhanced Glaucoma Risk Modeling and Patient Trajectory Analysis</b><br /></p>
  </li>

</ol>

<h2>Short Papers</h2>
<ol>
  <li>
    <p><a href="https://openreview.net/forum?id=MNxUWFZbkI">[Oral]</a> <b>PopAlign: Population-Level Alignment for Fair Text-to-Image Generation</b><br />
    <!-- Shufan Li, Aditya Grover, Harkanwar Singh<br/> -->
    </p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=H13U2WikCz">[Oral]</a> <b>Consistency-diversity-realism Pareto fronts of conditional image generative models</b><br />
    <!-- Pietro Astolfi, Melissa Hall, Jakob Verbeek, Marlene Careil, Oscar MaÃ±as, Matthew J. Muckley, Adriana Romero-Soriano, Michal Drozdzal<br/> --></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=P9hTkOJ1wW">[Paper]</a> <b>Trust but Verify: Reliable VLM evaluation in-the-wild with program synthesis</b><br />
    <!-- Viraj Uday Prabhu, Senthil Purushwalkam, Jieyu Zhang, An Yan, Caiming Xiong, Ran Xu<br/> --></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=95Bh0yjU9I">[Poster]</a> <b>You Never Know: Quantization Induces Inconsistent Biases in Vision-Language Foundation Models</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=pt2d3SlXQP">[Poster]</a> <b>Aligning to What? Limits to RLHF Based Alignment</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=nM75a1CzOI">[Poster]</a> <b>Exploring Intrinsic Fairness in Stable Diffusion</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=4tsFFBvRzX">[Poster]</a> <b>Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision Language Models</b><br /></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=4zbj329wz2">[Poster]</a> <b>Just rephrase it! Uncertainty estimation in closed-source language models via multiple rephrased queries</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=xFr6JT2hpy">[Poster]</a> <b>Position Paper: Protocol Learning, Decentralized Frontier Risk and the No-Off Problem</b><br /></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=Mm2mB1jHZK">[Poster]</a> <b>Comparison Visual Instruction Tuning</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=Dbuzp0DjgL">[Poster]</a> <b>Attention Shift: Steering AI Away from Unsafe Content</b><br /></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=yAPPj6nGwJ">[Poster]</a> <b>Towards Secure and Private AI: A Framework for Decentralized Inference</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=6O8wxERhSF">[Poster]</a> <b>WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models</b><br /></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=9NLRpwfLnT">[Poster]</a> <b>The Multi-faceted Monosemanticity in Multimodal Representations</b><br /></p>
  </li>

  <li>
    <p><a href="https://openreview.net/forum?id=EPa0udvXJE">[Poster]</a> <b>Adversarial Robust Deep Reinforcement Learning is Neither Robust Nor Safe</b><br /></p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=63T4BtNNOM">[Poster]</a> <b>Probabilistic Active Few-Shot Learning in Vision-Language Models</b><br />
    </p>
  </li>
</ol>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/%20/"></data>

    <div class="wrapper">

        <h2 class="footer-heading">Contact Us</h2>

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list"><ul class="social-media-list"></ul>
<li><a class="u-email" href="mailto:workshop.rbfm@gmail.com, maitreya.patel@asu.edu, ckim79@asu.edu">workshop.rbfm@gmail.com, maitreya.patel@asu.edu, ckim79@asu.edu</a></li></ul>
            </div>

            <div class="footer-col footer-col-2">
            </div>

            <div class="footer-col footer-col-3">
                <p>Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models.</p>
            </div>
        </div>

    </div>

</footer></body>

</html>
